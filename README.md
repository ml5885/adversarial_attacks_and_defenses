# Adversarial Attacks and Defenses

Project code for Programming Assignment 1 for 15-783: Trustworthy AI - Theory & Practice (Fall 2025).

## Repository structure

- `part_1.py` - Part 1 experiments (find smallest epsilon that breaks an ImageNet classifier).
- `part_2.py` - Part 2 experiments (adversarial training on MNIST).
- `part_3.py` - Part 3 experiments (GCG-style adversarial suffix optimization for LMs).
- `model.py` - Model definitions (ResNet-18 for ImageNet, simple CNN for MNIST).
- `train.py` - Training and evaluation code for MNIST models.
- `attacks/`
  - `fgsm.py` - FGSM attack
  - `pgd.py` - PGD attack and helpers
  - `loss.py` - CE/CW loss utilities
- `models/`
  - `resnet18_l2_eps0.ckpt` - ResNet-18 model trained without any adversarial training.
- `tests/`
  - `demo.py` - Single-image demo that generates example adversarial images.
- `requirements.txt` - Python dependencies.
- `gcg/`
  - `nanogcg.py` - Vendorized nanoGCG
  - `utils.py` - Helpers (tokenizer padding, mellowmax, batch-size finder, token filters).
- `behaviors.json` - Small list of red-team behaviors from the HarmBench validation set for training/testing suffix universality.

## Setup

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Part 1 - ImageNet: Smallest Epsilon That Breaks the Model

Runs PGD on 100 correctly classified val images with:

- Attacks: untargeted and targeted (random non-true target per image).
- Norms: L-inf and L2.
- Losses: Cross-Entropy (CE) and Carlini-Wagner margin (CW).
- Hyperparameters: PGD steps = 40, step size = epsilon/4.
- epsilon grids:
  - L-inf: {0, 1/255, ..., 8/255}
  - L2: 10 points in [0, 3.0]
- If ASR < 100% at the grid max, the sweep auto-extends epsilon.

Run:

```bash
python3 part_1.py
```

Outputs (saved to `results/part_1/`):

- Tables: median epsilon\* for each setting printed to stdout.
- Plots: `asr_untargeted_linf.png`, `asr_untargeted_l2.png`, `asr_targeted_linf.png`, `asr_targeted_l2.png`.
- Example attacked image: `example_attack.png`.

Example targeted PGD (L-2, CE) generated by `tests/demo.py`:

![Example PGD L-2 targeted CE](pgd_l2_targeted_ce.png)

## Part 2 - Adversarial Training on MNIST

Trains three MNIST models and reports robustness via FGSM accuracy:

- Baseline (standard training on clean data).
- FGSM adversarial training (epsilon=0.3).
- TRADES (beta=6.0, 1-step KL-FGSM, epsilon=0.3 for inner step).

Evaluation: train for 100k steps, then test accuracy under FGSM at epsilons {0.0, 0.1, 0.2, 0.3}. Results are saved to CSV and a plot is generated.

Run:

```bash
python3 part_2.py
```

Key options (defaults in parentheses):

- `--max-steps` (100000): optimisation steps per training regime.
- `--log-every` (500): logging frequency in steps.
- `--output-dir` (`results/part_2`): directory for outputs.
- `--analysis`: load results from CSV and only generate the plot.
- `--results-csv` (`<output-dir>/results.csv`): path to CSV file.

Outputs (under `results/part_2/` by default):

- `results.csv`: rows with columns `[regime, epsilon, accuracy]`.
- `accuracy_vs_epsilon.png`: line plot comparing Baseline, FGSM-trained, and TRADES across epsilons.
- Tables of accuracies per epsilon for each regime are printed to stdout.

## Part 3 - Adversarial Suffixes for LMs (GCG)

We implement GCG-style suffix optimization for chat LMs (vendorized and adapted from nanoGCG), including:

- Single-behavior optimization (target a specific behavior/target pair).
- Multi-behavior universality (learn one shared suffix that generalizes across 10 behaviors).
- Optional features: prefix KV-cache reuse, token filtering to avoid retokenization drift, mellowmax objective, and draft-model probe sampling (single-behavior). Multi-behavior currently does not use probe sampling.

Models:

- Defaults include Mistral 7B Instruct v0.2 and LLaMA 3.2 1B Instruct. You can pass any HF chat model that supports `apply_chat_template`.

Run Task 1 (single-model attack on one behavior):

```bash
python3 part_3.py --task task1 --model-id "meta-llama/Llama-3.2-1B-Instruct" --output-dir results/part_3/task1_run
```

- Behavior and target are taken from the first entry in `behaviors.json`.
- Logs a checkpointed loss curve and highlights the first non-refusal step.

Outputs (under your `--output-dir`/task1):

- `task1_loss_log.csv` with columns `[iteration, loss, first_non_refusal_here]`.
- `task1_loss_curve.png` plotting loss vs. iterations and first success marker.
- `task1_results.txt` containing default model output, final suffix, and final attacked output.

Run Task 2 (universality via behavior ensembling, one shared suffix):

```bash
python3 part_3.py --task task2 --model-id "mistralai/Mistral-7B-Instruct-v0.2" --output-dir results/part_3/task2_run
```

- Trains on 10 behaviors from `behaviors.json` and evaluates on 10 held-out HarmBench prompts that the model refuses by default.
- Success = non-refusal (heuristic keyword check).

Outputs (under your `--output-dir`/task2):

- `task2_asr_log.csv` with columns `[iteration, asr_val, asr_test]` where `asr_val` is train behaviors and `asr_test` is held-out prompts.
- `task2_asr_plot.png` with ASR vs. iterations for train and test sets.

Key options (defaults in parentheses):

- `--model-id` (mistralai/Mistral-7B-Instruct-v0.2): HF model to attack.
- `--num-steps` (500), `--search-width` (512), `--topk` (256), `--seed` (42).
- `--verbosity` (WARNING): logging level for GCG internals.
- `--analysis`: replot from existing CSV logs without running optimization.

## References

Our attack implementations are heavily adapted from:

- [MNIST Challenge](https://github.com/MadryLab/mnist_challenge)
- [CleverHans](https://github.com/cleverhans-lab/cleverhans)
- [nanoGCG](https://github.com/GraySwanAI/nanoGCG)
